
■ 타이타닉 로지스틱 회귀


### 기본 라이브러리 불러오기
import pandas as pd
import seaborn as sns

'''
[Step 1] 데이터 준비/ 기본 설정
'''

# load_dataset 함수를 사용하여 데이터프레임으로 변환
df = sns.load_dataset('titanic')

#  IPython 디스플레이 설정 - 출력할 열의 개수 한도 늘리기
pd.set_option('display.max_columns', 15)
print(df.head())


# ### 1. 데이터셋 설명(kaggle의 데이터셋 기준)

# - PassengerId : 승객 번호
# - Survived : 생존여부(1: 생존, 0 : 사망)
# - Pclass : 승선권 클래스(1 : 1st, 2 : 2nd ,3 : 3rd)
# - Name : 승객 이름
# - Sex : 승객 성별
# - Age : 승객 나이 
# - SibSp : 동반한 형제자매, 배우자 수
# - Patch : 동반한 부모, 자식 수
# - Ticket : 티켓의 고유 넘버
# - Fare 티켓의 요금
# - Cabin : 객실 번호
# - Embarked : 승선한 항구명(C : Cherbourg, Q : Queenstown, S : Southampton)

# ### 결측치 확인


print(df.isnull().sum())

print(df.info())



'''
[Step 2] 데이터 탐색/ 전처리
'''

# NaN값이 많은 deck 열을 삭제, embarked와 내용이 겹치는 embark_town 열을 삭제
rdf = df.drop(['deck', 'embark_town'], axis=1)  
print(rdf.head())


# age 열에 나이 데이터가 없는 모든 행을 삭제 - age 열(891개 중 177개의 NaN 값)
rdf = rdf.dropna(subset=['age'], how='any', axis=0)  
print(rdf.head())


# embarked 열의 NaN값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기
most_freq = rdf['embarked'].value_counts(dropna=True).idxmax()   
print(most_freq)

rdf['embarked'].fillna(most_freq, inplace=True)
print(rdf.head())



'''
[Step 3] 분석에 사용할 속성을 선택
'''

# 분석에 활용할 열(속성)을 선택 
ndf = rdf[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'embarked']]
print(ndf.head())


# 원핫인코딩 - 범주형 데이터를 모형이 인식할 수 있도록 숫자형으로 변환
onehot_sex = pd.get_dummies(ndf['sex'])
ndf = pd.concat([ndf, onehot_sex], axis=1)

onehot_embarked = pd.get_dummies(ndf['embarked'], prefix='town')
ndf = pd.concat([ndf, onehot_embarked], axis=1)

ndf.drop(['sex', 'embarked'], axis=1, inplace=True)
print(ndf.head())



'''
[Step 4] 데이터셋 구분 - 훈련용(train data)/ 검증용(test data)
'''

# 속성(변수) 선택
X=ndf[['pclass', 'age', 'sibsp', 'parch', 'female', 'male', 
       'town_C', 'town_Q', 'town_S']]  #독립 변수 X
y=ndf['survived']                      #종속 변수 Y


print(X.head())


# 설명 변수 데이터를 정규화(normalization)
from sklearn import preprocessing
X = preprocessing.StandardScaler().fit(X).transform(X)
print(X)


# train data 와 test data로 구분(7:3 비율)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) 

print('train data 개수: ', X_train.shape)
print('test data 개수: ', X_test.shape)
print('\n')



'''
[Step 5] 로지스틱 분류 모형 - sklearn 사용
'''

# sklearn 라이브러리에서 로지스틱 분류 모형 가져오기
from sklearn.linear_model import LogisticRegression

# 모형 객체 생성 (kernel='rbf' 적용)
logi_model = LogisticRegression()
logi_model
# 설명: 적절한 매개변수 C값과 gamma 값을 찾는게 중요한다 
# C 가 너무 크면 훈련 데이터는 잘 분류하지만 오버피팅이 발생하게 된다.
# gamma 매개변수는 결정경계의 곡률을 조정하는 매개변수인데
# gamma 가 너무 크면 훈련 데이터는 잘 분류하지만 오버피팅이 발생 할 수 있습니다.


'''
[Step 6] train data를 가지고 모형 학습
'''

# train data를 가지고 모형 학습
logi_model.fit(X_train, y_train)   


'''
[Step 7] test data를 가지고 y_hat을 예측 (분류) 
'''

# test data를 가지고 y_hat을 예측 (분류) 
y_hat = logi_model.predict(X_test)

print(y_hat[0:10])
print(y_test.values[0:10])
print('\n')


'''
[Step 8] 모형 성능 평가 - Confusion Matrix 계산
'''

# 모형 성능 평가 - Confusion Matrix 계산
from sklearn import metrics 
svm_matrix = metrics.confusion_matrix(y_test, y_hat)  
print(svm_matrix)
print('\n')


# 모형 성능 평가 - 평가지표 계산
svm_report = metrics.classification_report(y_test, y_hat)            
print(svm_report)


결과:
<class 'pandas.core.frame.DataFrame'>
Int64Index: 714 entries, 0 to 890
Data columns (total 13 columns):
 #   Column      Non-Null Count  Dtype   
---  ------      --------------  -----   
 0   survived    714 non-null    int64   
 1   pclass      714 non-null    int64   
 2   sex         714 non-null    object  
 3   age         714 non-null    float64 
 4   sibsp       714 non-null    int64   
 5   parch       714 non-null    int64   
 6   fare        714 non-null    float64 
 7   embarked    714 non-null    object   <----  문자형
 8   class       714 non-null    category  <--- R 의 factor(문자형+level) 와 같음
 9   who         714 non-null    object  
 10  adult_male  714 non-null    bool    
 11  alive       714 non-null    object  
 12  alone       714 non-null    bool    
dtypes: bool(2), category(1), float64(2), int64(4), object(4)
memory usage: 63.6+ KB
None

              precision    recall  f1-score   support

           0       0.81      0.87      0.84       125
           1       0.80      0.72      0.76        90

    accuracy                           0.81       215
   macro avg       0.81      0.80      0.80       215
weighted avg       0.81      0.81      0.81       215


■ 데이터 표준화

실무에서 접하는 데이터셋은 다양한 사람들의 손에 거쳐서 만들어진다.
여러곳에 수집한 자료들은 대소문자 구분, 약칭 활용 등 여러가지 원인에 의해서
다양한 형태로 표현된다. 잘 정리된 것으로 보이는 자료를 자세히 들여다보면
서로 다른 단위가 섞여있거나 같은 대상을 다른 형식으로 표현하는 경우가 의외로 많다.
그래서 단위환산을 통해서 같은 단위 형식으로 변환을 해줄 필요가 있다.

예: 모전자에서 모제품의 판매대수를 미리 예측하여 생산라인에 다음달에 몇대가 판매가
될거라는 것을 예측해야하는 머신러닝 모델을 만들어야 하는 경우에 전세계에서 엑셀파일을
보내오는데 나라마다 단위형식이 다르다. (화폐단위, 연비단위, …)


■ 머신러닝 데이터 분석을 하기 위한 단계?

 1. 데이터 불러오기
                파생변수 추가
 2. 데이터 탐색 및 전처리   --->   결측치 처리
 3. 범주형 변수 더미변수(인코딩)로 변환 (파이썬)
 4. 데이터 정규화 또는 표준화  --->  이상치에 민감하지 않도록 단위를 일정하게 조정
 5. 훈련 데이터와 테스트 데이터 분
 6. 머신러닝 모델 생성
 7. 훈련 데이터로 머신러닝 모델 훈련
 8. 머신러닝 모델 평가
 9. 머신러닝 모델 성능 개선


문제35. women_child 파행변수를 추가하고 다시 학습시켜서 모델의 성능을 확인하시오 !

답:
### 기본 라이브러리 불러오기
import pandas as pd
import seaborn as sns

'''
[Step 1] 데이터 준비/ 기본 설정
'''

# load_dataset 함수를 사용하여 데이터프레임으로 변환
df = sns.load_dataset('titanic')

#  IPython 디스플레이 설정 - 출력할 열의 개수 한도 늘리기
pd.set_option('display.max_columns', 15)
print(df.head())


#%%
# 여자와 아이에 대한 파생변수 추가
mask = ( df.age < 10 ) | ( df.sex=='female')
mask.astype(int) # True 를 1 로 변경하고 False 를 0 으로 변경
df['women_child'] = mask.astype(int)
print(df)


# ### 결측치 확인
print(df.isnull().sum())

print(df.info())


'''
[Step 2] 데이터 탐색/ 전처리
'''

# NaN값이 많은 deck 열을 삭제, embarked와 내용이 겹치는 embark_town 열을 삭제
rdf = df.drop(['deck', 'embark_town'], axis=1)  

# age 열에 나이 데이터가 없는 모든 행을 삭제 - age 열(891개 중 177개의 NaN 값)
rdf = rdf.dropna(subset=['age'], how='any', axis=0)  


# embarked 열의 NaN값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기
most_freq = rdf['embarked'].value_counts(dropna=True).idxmax()   
rdf['embarked'].fillna(most_freq, inplace=True)

print(rdf.info())


'''
[Step 3] 분석에 사용할 속성을 선택
'''

# 분석에 활용할 열(속성)을 선택 
ndf = rdf[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'embarked', 'women_child']]
print(ndf.head())

# 원핫인코딩 - 범주형 데이터를 모형이 인식할 수 있도록 숫자형으로 변환
onehot_sex = pd.get_dummies(ndf['sex'])
ndf = pd.concat([ndf, onehot_sex], axis=1)

onehot_embarked = pd.get_dummies(ndf['embarked'], prefix='town')
ndf = pd.concat([ndf, onehot_embarked], axis=1)

ndf.drop(['sex', 'embarked'], axis=1, inplace=True)


'''
[Step 4] 데이터셋 구분 - 훈련용(train data)/ 검증용(test data)
'''

# 속성(변수) 선택
X=ndf[['pclass', 'age', 'sibsp', 'parch', 'women_child', 'female', 'male', 
       'town_C', 'town_Q', 'town_S']]  #독립 변수 X
y=ndf['survived']                      #종속 변수 Y

print(X.head())



# 설명 변수 데이터를 정규화(normalization)
from sklearn import preprocessing
X = preprocessing.StandardScaler().fit(X).transform(X)
print(X)


# train data 와 test data로 구분(7:3 비율)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) 

print('train data 개수: ', X_train.shape)
print('test data 개수: ', X_test.shape)
print('\n')


'''
[Step 5] 로지스틱 분류 모형 - sklearn 사용
'''

# sklearn 라이브러리에서 로지스틱 분류 모형 가져오기
from sklearn.linear_model import LogisticRegression

# 모형 객체 생성 (kernel='rbf' 적용)
logi_model = LogisticRegression()
logi_model


'''
[Step 6] train data를 가지고 모형 학습
'''

# train data를 가지고 모형 학습
logi_model.fit(X_train, y_train)   


'''
[Step 7] test data를 가지고 y_hat을 예측 (분류) 
'''

# test data를 가지고 y_hat을 예측 (분류) 
y_hat = logi_model.predict(X_test)

print(y_hat[0:10])
print(y_test.values[0:10])
print('\n')


'''
[Step 8] 모형 성능 평가 - Confusion Matrix 계산
'''

# 모형 성능 평가 - Confusion Matrix 계산
from sklearn import metrics 
svm_matrix = metrics.confusion_matrix(y_test, y_hat)  
print(svm_matrix)
print('\n')


# 모형 성능 평가 - 평가지표 계산
svm_report = metrics.classification_report(y_test, y_hat)            
print(svm_report)

결과:
              precision    recall  f1-score   support

           0       0.85      0.90      0.88       125
           1       0.85      0.79      0.82        90

    accuracy                           0.85       215
   macro avg       0.85      0.84      0.85       215
weighted avg       0.85      0.85      0.85       215



■ 성능 개선을 위해 데이터를 시각화 해서 데이터에서 스토리를 뽑아내는 방법

** 범주형 데이터를 처리하는 방법

 1. 더미변수로 처리 : 숫자 0과 1로 표현하는 것
예: 여자 또는 아니는 1이고 아니면 0 으로 표현

 2. 구간 분할로 처리 : 일정한 구간으로 나눔
예: 미세먼지의 농도는 연속형 수치형 데이터여서 누군가가 나에게 오늘 미세먼지 농도를
    물어봤을때 63 ㎍/㎥ 입니다 라고 하면 전문가가 아니면 알아듣기 어렵습니다.
    그냥 좋음, 보통, 나쁨, 매우나쁨 이라고 알려주는게 훨씬 이해하기 편합니다.

예: 타이타닉 데이터도 운임 컬럼을 low, medium-low, medium, high 구간 분할 하면
    기계가 훨씬 더 잘 이해해서 성능이 더 올라가게 됩니다.


□ 구간분할 설명을 위해 필요한 준비 스크립트

# CSV 파일을 데이터프레임으로 변환
df = pd.read_csv('D:\\data\\auto-mpg.csv', header=None)

# 열 이름 지정
df.columns = ['mpg','cylinders','displacement','horsepower','weight',
 'acceleration','model year','origin','name']

# 데이터 살펴보기
print(df.head())
print('\n')

# IPython 디스플레이 설정 - 출력할 열의 개수 한도 늘리기
pd.set_option('display.max_columns', 10)
print(df.head())
print('\n')
'''
[Step 2] 데이터 탐색
'''
# 데이터 자료형 확인
print(df.info())
print('\n')

# 설명:  horsepower 가 object 여서 수치형 데이터로 변경해야합니다. 
#  전처리가 필요한 문자가 포함되어있어서 object 문자형으로 출력되고 있습니다. 

# 데이터 통계 요약정보 확인
print(df.describe())
print('\n')

# 설명:  horsepower 가 빠졌음을 확인하세요 ~  통계요약정보를 출력하려면
# 숫자형 데이터여야 출력될 수 있습니다. 

# mpg 는 mile per gallon 의 약자로 영국과 미국에서는 한국과는 달리
# 갤런당 마일 단위로 연비를 표시합니다.
# 한국은 리터당 킬로미터(km/L) 단위로 표시한다.
# mpg 열을 한국에서 사용하는 km/L 로 변환해줘야 합니다. 
# 1 갤런이 3.78541 이다. 그리고 1마일이 1.60934 km 이다.
# 그렇다면 1 mpg(mile per  gallon) 은 ?

# print  ( 1.60934 /  3.78541 )  # 0.425  km/L 

# horsepower 열의 자료형 변경 (문자열 ->숫자)
print(df['horsepower'].unique()) # horsepower 열의 고유값 확인
print('\n')
df['horsepower'].replace('?', np.nan, inplace=True) # '?'을 np.nan으로 변경

df.dropna(subset=['horsepower'], axis=0, inplace=True) # 누락데이터 행을 삭제

df['horsepower'] = df['horsepower'].astype('float') # 문자열을 실수형으로 변환
print(df.describe()) # 데이터 통계 요약정보 확인


# 판다스의 cut() 함수를 이용해서 horsepower 를 구간 분할하기

count, bin_dividers = np.histogram(df.horsepower, bins=3)
print(count)   # 각 구간에 속하는 값의 개수

결과: [257 103  32]

print(bin_dividers)  # 경계값 리스트 

결과: [ 46.         107.33333333 168.66666667 230.        ]

설명: 10 - 46 = -36 61, 168 - 107=61 61, 230 - 168 = 62 61

import pandas as pd
bin_names = ['저출력', '보통출력', '고출력']
df['hp_bin'] = pd.cut( x = df.horsepower, bins=bin_dividers, labels=bin_names)
print(df)

설명: 이처럼 연속형 변수를 일정한 구간으로 나눠 각 구간을 범주형 이산 변수로
	변환하는 과정을 구간분할(binning) 이라고 합니다.
	판다스의 cut() 함수를 이용하면 연속 데이터를 여러 구간으로 나누고 
	범주형 데이터로 변환할 수 있니다.
	

문제36. emp 데이터 프레임의 월급을 3개로 구간 분할 하는데 고소득, 중간소득, 저소득으로
	   나눠서 출력하는 sal_bin 이라는 파생변수를 추가하시오 ~

import pandas as pd
import numpy as np

emp = pd.read_csv('c:\\data\\emp.csv')
count, bin_dividers = np.histogram(emp.sal, bins=3)
print(count)   # 각 구간에 속하는 값의 개수
print(bin_dividers) # 경계값 리스트 
bin_names = ['저소득', '중간소득', '고소득']
emp['sal_bin'] = pd.cut( x = emp.sal, bins=bin_dividers, labels=bin_names, include_lowest=True)
print(emp)


문제37. 여자와 아이에 대한 파생변수를 추가해서 0.85 까지 올렸던 코드를 로지스틱 회귀가
	   아니라 앙상블 기법이 추가된 랜덤포레스트로 수행해서 정확도가 더 올라가는지
	   확인하시오
	
### 기본 라이브러리 불러오기
import pandas as pd
import seaborn as sns

'''
[Step 1] 데이터 준비/ 기본 설정
'''

# load_dataset 함수를 사용하여 데이터프레임으로 변환
df = sns.load_dataset('titanic')


# 여자와 아이에 대한 파생변수 추가
mask = ( df.age < 10 ) | ( df.sex=='female')
mask.astype(int) # True 를 1 로 변경하고 False 를 0 으로 변경
df['women_child'] = mask.astype(int)
print(df)



'''
[Step 2] 데이터 탐색/ 전처리
'''

# NaN값이 많은 deck 열을 삭제, embarked와 내용이 겹치는 embark_town 열을 삭제
rdf = df.drop(['deck', 'embark_town'], axis=1)  

# age 열에 나이 데이터가 없는 모든 행을 삭제 - age 열(891개 중 177개의 NaN 값)
rdf = rdf.dropna(subset=['age'], how='any', axis=0)  


# embarked 열의 NaN값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기
most_freq = rdf['embarked'].value_counts(dropna=True).idxmax()   
rdf['embarked'].fillna(most_freq, inplace=True)

print(rdf.info())


'''
[Step 3] 분석에 사용할 속성을 선택
'''

# 분석에 활용할 열(속성)을 선택 
ndf = rdf[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'embarked','women_child']]
ndf.head()

# 원핫인코딩 - 범주형 데이터를 모형이 인식할 수 있도록 숫자형으로 변환
onehot_sex = pd.get_dummies(ndf['sex'])
ndf = pd.concat([ndf, onehot_sex], axis=1)

onehot_embarked = pd.get_dummies(ndf['embarked'], prefix='town')
ndf = pd.concat([ndf, onehot_embarked], axis=1)

ndf.drop(['sex', 'embarked'], axis=1, inplace=True)
ndf.head()

'''
[Step 4] 데이터셋 구분 - 훈련용(train data)/ 검증용(test data)
'''

# 속성(변수) 선택
X=ndf[['pclass', 'age', 'sibsp', 'parch', 'female', 'male', 
       'town_C', 'town_Q', 'town_S','women_child']]  #독립 변수 X
y=ndf['survived']                      #종속 변수 Y

# 설명 변수 데이터를 정규화(normalization)
from sklearn import preprocessing
X = preprocessing.StandardScaler().fit(X).transform(X)
print(X)

# train data 와 test data로 구분(7:3 비율)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) 

print('train data 개수: ', X_train.shape)
print('test data 개수: ', X_test.shape)
print('\n')

'''
[Step 5] 랜덤 포레스트 분류 모형 - sklearn 사용
'''

# sklearn 라이브러리에서 랜덤포레스트 분류 모형 가져오기
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# 모형 객체 생성 (kernel='rbf' 적용)
tree_model = RandomForestClassifier( n_estimators=100,
                                                   oob_score=True,
                                                    random_state= 9 )  
tree_model


'''
[Step 6] train data를 가지고 모형 학습
'''

# train data를 가지고 모형 학습
tree_model.fit(X_train, y_train)   


'''
[Step 7] test data를 가지고 y_hat을 예측 (분류) 
'''

# test data를 가지고 y_hat을 예측 (분류) 
y_hat = tree_model.predict(X_test)

print(y_hat[0:10])
print(y_test.values[0:10])
print('\n')


'''
[Step 8] 모형 성능 평가 - Confusion Matrix 계산
'''

# 모형 성능 평가 - Confusion Matrix 계산
from sklearn import metrics 
svm_matrix = metrics.confusion_matrix(y_test, y_hat)  
print(svm_matrix)
print('\n')


# 모형 성능 평가 - 평가지표 계산
svm_report = metrics.classification_report(y_test, y_hat)            
print(svm_report)


결과:
            precision    recall  f1-score   support

           0       0.80      0.83      0.82       125
           1       0.75      0.71      0.73        90

    accuracy                           0.78       215
   macro avg       0.78      0.77      0.77       215
weighted avg       0.78      0.78      0.78       215



문제38. (오늘의 마지막 문제) 로지스틱 회귀로 시본의 타이타닉 분류 모델을 만드는데
	    여자와 아이 파생변수 말고 아래의 그래프를 보고 더 좋은 파생변수를 생각해서
	    학습시키고 정확도 출력되는 화면을 올리시오 ~


