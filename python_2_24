■ 복습

   1. 파이썬으로 knn 구현하기
   2. 파이
   3. 파이
   4. 파이썬으로 regresiion 구현하기 (단순회귀 분석, 다중 회귀 분석)


■ 머신러닝 데이터 분석 5가지 단계 ( 빅 피쳐 ) 

   1. 데이터 수집과 설명 : pandas 를 사용 
   2. 데이터 탐색 및 시각화 : pandas, matplotlib, seaborn 사용
   3. 머신러닝 모델 훈련 : sklearn 사용
   4. 머신러닝 모델 평가 : pandas 사용
   5. 머신러닝 모델 성능 개선 : pandas 를 사용 ( 파생변수 생성 )

※ 어제 마지막 문제의 결정계수를 확인하고 성능을 더 높이시오. 

성능 개선 방법 : 단순 회귀 ---> 다항회귀로 변경해서 성능을 올린다. 

   1. 단순회귀 : 독립변수 한개에 종속변수 한 개 ( 선형 회귀선 ) 
   2. 다항회귀 : 독립변수 한개에 종속변수 한 개 ( 비선형 회귀선 )
   3. 다중회귀 : 종속변수에 영향을 주는 독립변수가 여러개인 경우.


문제. 첫번째 예제 단순회귀분석의 결과 그래프(무게와 연비간의 예측값과 실제값의 비교)를 보면 실제값은 왼쪽으로 편향되어있고 예측값을 반대로 오른쪽으로 편중되는 경향을 보인다. 따라서 독립변수(weight) 과 종속변수(mpg) 사이의 선형관계가 있지만, 모형의 오차를 더 줄일 필요가 있어 보인다. 앞에서 본 산포도를 보면 직선보다는 곡선이 더 적합해 보인다.  비선형 회귀분석을 통해 모형의 정확도를 더 높이시오.


 어제 것과 차이 나는거는 step5 부터
$$ 쥬피터 코드 +

문제19. 어제 마지막 문제로 풀었던 체중과 키와의 단순 선형 회귀 분석 결과의 테스트 데이터에 대한 결정계수는 0.86 이었습니다. 그렇다면 이번에는 다항 회귀로 비선형 회귀선을 만들어 결정계수를 더 올리시오.

미승이꺼 : 단순 : 0.8657
       다항 :  0.9263
   
내꺼 : 단순 :  0.6615877402621955
        다항 :  0.6693211632386808 

내꺼 R 스퀘어는 왜이러지 ;; 확인하고 수정하기

$$ 쥬피터 코드 

----------------------------------------------------------

■ 6. Multi Regression (다중회귀)

종속변수에 영향을 주는 독립변수가 여러개인 경우

예제1. 미국 우주 왕복선 폭파 원인
예제2. 미국 대학교 입학점수에 영향을 미치는 과목 분석
예제3. 미국 국민 의료비에 영향을 주는 요소 분석

▩ 예제1. 미국 우주 왕복선 폭파 원인

o형링의 손상이 온도, 압력, 비행기번호 이 3가지 중에 어떤것이 더 영향이 큰지 ?

import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd

df = pd.read_csv("c:\\data\\challenger.csv", engine='python', encoding='CP949')
print(df)

# 다중 회귀 분석 코드

model = smf.ols( formula='distress_ct ~ temperature + field_check_pressure + flight_num',
                data = df)

result = model.fit()
print( result.summary() )

결과:
    distress_ct  temperature  field_check_pressure  flight_num
0             0           66                    50           1
1             1           70                    50           2
2             0           69                    50           3
3             0           68                    50           4
4             0           67                    50           5
5             0           72                    50           6
6             0           73                   100           7
7             0           70                   100           8
8             1           57                   200           9
9             1           63                   200          10
10            1           70                   200          11
11            0           78                   200          12
12            0           67                   200          13
13            2           53                   200          14
14            0           67                   200          15
15            0           75                   200          16
16            0           70                   200          17
17            0           81                   200          18
18            0           76                   200          19
19            0           79                   200          20
20            2           75                   200          21
21            0           76                   200          22
22            1           58                   200          23
                            OLS Regression Results                            
==============================================================================
Dep. Variable:            distress_ct   R-squared:                       0.360
Model:                            OLS   Adj. R-squared:                  0.259
Method:                 Least Squares   F-statistic:                     3.563
Date:                Wed, 24 Feb 2021   Prob (F-statistic):             0.0337
Time:                        11:04:05   Log-Likelihood:                -17.308
No. Observations:                  23   AIC:                             42.62
Df Residuals:                      19   BIC:                             47.16
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
========================================================================================
                           coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------------
Intercept                3.5271      1.307      2.699      0.014       0.791       6.263
temperature             -0.0514      0.018     -2.802      0.011      -0.090      -0.013
field_check_pressure     0.0018      0.003      0.517      0.611      -0.005       0.009
flight_num               0.0143      0.035      0.407      0.689      -0.059       0.088
==============================================================================
Omnibus:                       17.300   Durbin-Watson:                   2.392
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               18.847
Skew:                           1.686   Prob(JB):                     8.08e-05
Kurtosis:                       5.881   Cond. No.                     1.98e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.98e+03. This might indicate that there are
strong multicollinearity or other numerical problems.


분석결과 설명: o 형링 파손에 영향을 주는 가장 큰 독립변수는 온도 입니다.
		    그 다음이 비행기 노후화를 나타내는 비행기 번호 입니다.

회귀식 : y = 3.5271 - 0.0514 * x1 + 0.0018 * x2 + 0.0143 * x3


문제20. statsmodels 패키지를 이용해서 방금 다중 회귀 분석을 해보았는데 이번에는 중요한
   독립변수인 temperature 만 이용해서 단순회귀 분석을 진행하고 분석된 결과를 출력하세요

종속변수 : distress_ct (o 형링 파손수)
독립변수 : temperature(온도)

import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd

df = pd.read_csv("c:\\data\\challenger.csv", engine='python', encoding='CP949')
print(df)

# 단순 회귀 분석 코드

model = smf.ols( formula='distress_ct ~ temperature',
                data = df)

result = model.fit()
print( result.summary() )

결과: 
                                              :
                              
==================================================
                  coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept       3.6984      1.220      3.033      0.006       1.162       6.235
temperature    -0.0475      0.017     -2.725      0.013      -0.084      -0.011
==================================================

                                              :

회귀식 : y(distress_ct) = 3.6984 - 0.0475 * x1 (temperature)
           ↑                                         ↑
         파손수                                    온도

	  2.21 개                                 31도 F(화씨)
	  0.82 개                                 60도 F(화씨)
	  0.34 개                                 70도 F(화씨)

분석결과: 화씨 30도에서 발사하는게 화씨 60도에서 발사하는 것 보다 약 3배 더 위험하고
		화씨 70도에서 발사하는 것 보다 약 7~8배 더 위험합니다.


▩ 예제2. 미국 대학교 입학점수에 영향을 미치는 과목 분석

데이터 : sports.csv

종속변수 : acceptance

독립변수 : academic, sports, music

설명: 
분석요청 : 학과점수, 체육점수, 음악점수 중에 어떤 것이 더 입학 여부에 큰 영향을 주는지 ?
		3과목의 점수 단위가 과목마다 다릅니다. (예: 체중과 키 처럼)
		그래서 이런 경우에는 표준화를 하고 회귀분석을 해야합니다.

 1. 표준화를 안했을때 ? 

import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
from  sklearn.preprocessing import StandardScaler   # 표준화를 위해 필요

# 1. 데이터 불러오기
df = pd.read_csv("c:\\data\\sports.csv", engine='python', encoding='CP949') 
df.columns = ['stud_id', 'academic', 'sports', 'music', 'acceptance']   # 컬럼명을 지정합니다.
print(df)


# 2. 모델 생성하기
model = smf.ols(formula = 'acceptance ~ academic + sports + music', data = df)

result = model.fit()  # 모델 훈련
print( result.summary() )

결과:
===================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     11.4903      1.053     10.916      0.000       9.414      13.566
academic       0.1558      0.006     26.877      0.000       0.144       0.167
sports         0.5727      0.040     14.430      0.000       0.494       0.651
music          0.1046      0.023      4.465      0.000       0.058       0.151
===================================================

분석결과 : 표준화를 안했을때는 체육점수가 학과점수보다 더 영향력이 컸습니다.


 2. 표준화를 했을때 ?

import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
from  sklearn.preprocessing import StandardScaler   # 표준화를 위해 필요

# 1. 데이터 불러오기
df = pd.read_csv("c:\\data\\sports.csv", engine='python', encoding='CP949') 
print(df)

# 2. 표준화
scaler = StandardScaler()
scaler.fit(df)   # 표준화를 위해 df 데이터를 살펴본다.
df_scale = scaler.transform(df)  # 표준화 작업 수행한 결과를 df 에 할당
print(df_scale)   # np.array 로 구성

# 3. 판다스 데이터 프레임으로 구성합니다.
df_scale2 = pd.DataFrame(df_scale)
print(df_scale2.head())

# 4. 컬럼을 구성합니다.
df_scale2.columns = ['stud_id', 'academic', 'sports', 'music', 'acceptance']   # 컬럼명을 지정합니다.
print(df_scale2.head())

# 5. 회귀모델을 생성하고 summary 결과를 봅니다.
model = smf.ols(formula = 'acceptance ~ academic + sports + music', data = df_scale2)
result = model.fit()  # 모델 훈련
print( result.summary() )

결과: 
==================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept  -1.041e-17      0.022  -4.77e-16      1.000      -0.043       0.043
academic       0.6921      0.026     26.877      0.000       0.641       0.743
sports         0.4400      0.030     14.430      0.000       0.380       0.500
music          0.1511      0.034      4.465      0.000       0.084       0.218
==================================================

# 분석결과 : 학과점수가 체육점수보다 더 영향력이 큰 독립변수로 나타나고 있습니다.


▩ 예제3. 미국 국민 의료비에 영향을 주는 요소 분석

문제21. (점심시간 문제) R 을 활용한 머신러닝 수업때 회귀분석 할 때 사용했던
	   미국 의료비 데이터(insurance.csv)를 가지고 다중회귀분석을 하시오 !
	   회귀분석한 결과를 출력하세요 ~
	
	결정계수가 출력되는 결과 화면을 첨부해서 올리세요
	
	오후에 파생변수 추가해서 결정계수가 올라가는지 확인을 해야하므로
	꼭 점심시간 문제를 풀어서 올리세요 ~

종속변수 : expenses
독립변수 : age, sex, bmi, children, smoker, region

표준화하지 말고 진행하세요. 가족이 한명 늘어날수록 연간 의료비가 얼마나 
늘어나는지 보려면 표준화하면 안됩니다.


import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd

# 1. 데이터 불러오기
df = pd.read_csv("c:\\data\\insurance.csv", engine='python', encoding='CP949') 
print(df)


# 2. 모델 생성하기
model = smf.ols(formula = 'expenses ~ age + sex + bmi + children + smoker + region', data = df)

result = model.fit()  # 모델 훈련
print( result.summary() )

                            OLS Regression Results                            
==============================================================================
Dep. Variable:               expenses   R-squared:                       0.751
Model:                            OLS   Adj. R-squared:                  0.749
Method:                 Least Squares   F-statistic:                     500.9
Date:                Wed, 24 Feb 2021   Prob (F-statistic):               0.00
Time:                        12:07:08   Log-Likelihood:                -13548.
No. Observations:                1338   AIC:                         2.711e+04
Df Residuals:                    1329   BIC:                         2.716e+04
Df Model:                           8                                         
Covariance Type:            nonrobust                                         
=======================================================================================
                          coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------------
Intercept           -1.194e+04    987.811    -12.089      0.000   -1.39e+04      -1e+04
sex[T.male]          -131.3520    332.935     -0.395      0.693    -784.488     521.784
smoker[T.yes]        2.385e+04    413.139     57.723      0.000     2.3e+04    2.47e+04
region[T.northwest]  -352.7901    476.261     -0.741      0.459   -1287.095     581.515
region[T.southeast] -1035.5957    478.681     -2.163      0.031   -1974.648     -96.544
region[T.southwest]  -959.3058    477.912     -2.007      0.045   -1896.850     -21.762
age                   256.8392     11.899     21.586      0.000     233.497     280.181
bmi                   339.2899     28.598     11.864      0.000     283.187     395.393
children              475.6889    137.800      3.452      0.001     205.360     746.017
==============================================================================
Omnibus:                      300.499   Durbin-Watson:                   2.088
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              719.382
Skew:                           1.212   Prob(JB):                    6.14e-157
Kurtosis:                       5.652   Cond. No.                         311.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

분석결과:
sex[T.male] : -131.3520 --> 남성은 여성에 비해 매년 의료비가 131달러 적게 들거라 예상
smoker[T.yes] : 2.385e+04 --> 흡연자는 비흡연자보다 매년 의료비가 2.368x10^4=23,680 달러 
						비용이 더 든다
age : 256.8392  --> 나이가 일년씩 더해질 때 마다 평균적으로 의료비가 256달러 더 든다.
bmi : 339.2899  --> 비만지수가 증가할 때 마다 339달러 더 들거라 예상
children : 475.6889 --> 부양가족이 한명 더 늘어날 때 마다 연간 의료비가 475달러 더 든다.

지역별로는 북동지역이 북서, 남동, 남서에 비해 의료비가 더 든다.

결정계수 : 0.751


문제22. 비만인 사람은 의료비가 더 지출이 되는지 파생변수를 추가해서 확인하시오 !
	   bmi30 이라는 파생변수를 추가하는데 bmi 가 30 이상이면 1, 아니면 0 이라고 해서
	   컬럼을 하나 만드세요 ~
	
** 파생변수 추가 방법
 1. R 에서 df$bmi <- ifelse( bmi >= 30, 1, 0)
 2. 파이썬에서 df['bmi30'] = df['bmi'].apply(함수)

** 파이썬 함수를 생성하는데 입력값이 30 이상이면 1이고 아니면 0을 출력하는 함수를
   func_1 이라는 이름으로 생성하시오 !

import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd

# 1. 데이터 불러오기
df = pd.read_csv("c:\\data\\insurance.csv", engine='python', encoding='CP949') 
 
# 2. 파생변수 추가
def func_1(x):
    if x >= 30:
        return 1
    else:
        return 0

df['bmi30'] = df['bmi'].apply(func_1)
print(df)


문제23. 비만인 사람(bmi30) 을 분류하는 파생변수를 추가했으면 결정계수가 더 올라가는지 
	    확인하시오 !

# 3. 모델 생성하기
model = smf.ols(formula = 'expenses ~ age + sex + bmi + children + smoker + region + bmi30', data = df)

result = model.fit()  # 모델 훈련
print( result.summary() )

분석결과: R-squared 기존 0.751에서 0.756 으로 올라갔습니다.


문제24. 비만이면서 흡연까지 하면 의료비가 더 드는지 확인하시오 !

# 3. 모델 생성하기
model = smf.ols(formula = 'expenses ~ age + sex + bmi + children + smoker + region \
                + bmi30 + bmi30 * smoker', data = df)
result = model.fit()  # 모델 훈련
print( result.summary() )


R-squared : 0.864
bmi30 * smoker 의 회귀계수 : 1.979e+04 
설명 : 비만이면서 흡연까지 하게되면 연간 의료비가 19,790 달러 더 들거라 예상이 됩니다.


■ 다중공선성 확인을 파이썬으로 구현하기

  회귀분석에서 사용된 모형의 일부 독립변수가 다른 독립변수와의 상관정도가 아주 높아서
  회귀분석 결과에 부정적 영향을 미치는 현상을 말합니다.

  두 독립변수들끼리 서로에게 영향을 주고 있다면 둘 중 하나의 영향력을 검증할 때
  다른 하나의 영향력이 약해집니다.

예: 학업 성취도, 일 평균 음주량, 혈중 알코올 농도
         ↑
    종속변수

  팽창계수가 보통은 10보다 큰 것을 골라내고 까다롭게 하려면 5보다 큰 것을 골라냅니다.

  일 평균 음주량, 혈중 알코올 농도 둘 다 팽창계수가 높게 나온다면 둘중에 하나를 빼고
  아래와 같이 

  학업 성취도, 일 평균 음주량 ---> 회귀분석
  학업 성취도, 혈중 알코올 농도 ---> 회귀분석


예제: crab.csv : 게의 크기, 무게 등에 대한 데이터로 종속변수가 y 컬럼인데 0과 1로 분류하는
			데이터 입니다.

# 1. 데이터 불러오기
import pandas as pd

df = pd.read_csv("c:\\data\\crab.csv")
print(df.head())
print(df.y.unique())

# 2. 다중회귀분석을 하고 종속변수에 영향을 주는 독립변수들이 무엇인지 확인하기
import statsmodels.formula.api as ols

model = ols('y ~ sat + weight + width', data=df)
result = model.fit()
print( result.summary() )

결과:
================================================
                 coef       std err         t          P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.9366      0.500     -1.872      0.063      -1.924       0.051
sat            0.0971      0.009     11.018      0.000       0.080       0.115
weight        -0.0465      0.098     -0.475      0.635      -0.240       0.147
width          0.0535      0.026      2.023      0.045       0.001       0.106
================================================

분석결과: width는 종속변수에 영향을 주는 유의미한 독립변수로 나타나지만 weight는 
             그렇지 않게 보입니다. 별로 중요한 독립변수로 보고 있지 않습니다. 
             분석을 잘못할 수 있게 됩니다.


# 3. 팽창계수를 확인합니다.

from statsmodels.stats.outliers_influence import variance_inflation_factor

print( model.exog_names )   # 모델에서 분석한 독립변수들이 출력

variance_inflation_factor(model.exog, 1)  # 위의 출력된 독립변수중에 첫번째 컬럼의 
                                                     # 팽창계수 확인
variance_inflation_factor(model.exog, 2)
variance_inflation_factor(model.exog, 3)

결과:
1.15883687808578  : sat ~ weight + width
4.8016794240392375 : weight ~ sat + width
4.688660343641888 : width ~ sat + weight

weight 과 width 가 높은 팽창계수를 보이고 있습니다.






# 4. 위의 팽창계수가 높은 두개의 독립변수를 각각 따로따로 이용해서 모델을 생성합니다.

model1 = ols( 'y ~ sat + width', data=df )
print(model1.fit().summary())

결과:
================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.7600      0.334     -2.274      0.024      -1.420      -0.100
sat            0.0965      0.009     11.105      0.000       0.079       0.114
width          0.0426      0.013      3.285      0.001       0.017       0.068
================================================


model2 = ols( 'y ~ sat + weight', data=df )
print(model2.fit().summary())

결과:
===============================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0495      0.114      0.433      0.665      -0.176       0.275
sat            0.0976      0.009     10.982      0.000       0.080       0.115
weight         0.1260      0.049      2.598      0.010       0.030       0.222
===============================================

설명: 아까는 weight 가 중요하지 않은 독립변수였는데 width 를 빼고 분석해보니
	중요한 독립변수임이 확인이 되고 있습니다.


문제25. test_vif1.csv 를 내려받고 팽창계수를 확인하여 vif 지수가 높은 독립변수들이
	   무엇이 있는지 확인하시오 !

데이터 설명 : 아이큐, 공부시간, 시험점수로 되어있는 데이터입니다.

# 1. 데이터 불러오기
import pandas as pd

df = pd.read_csv("c:\\data\\test_vif1.csv", encoding = 'CP949')
print(df)

# 2. 다중회귀분석을 하고 종속변수에 영향을 주는 독립변수들이 무엇인지 확인하기
import statsmodels.formula.api as smf

model = smf.ols('시험점수 ~ 아이큐 + 공부시간', data=df)
result = model.fit()
#print( result.summary() )

# 3. 팽창계수를 확인합니다.

from statsmodels.stats.outliers_influence import variance_inflation_factor

print( model.exog_names )   # 모델에서 분석한 독립변수들이 출력

print(variance_inflation_factor(model.exog, 1))
print(variance_inflation_factor(model.exog, 2))


■ 내가 추가한 파생변수가 유의미한 파생변수인지 확인을 하고 여러 독립변수들 
    중에 불필요한 독립변수를 제거하고 필요한 독립변수들만 골라내서 회귀분석 
    할 때 사용하는 step 함수를 파이썬으로 구현하기

# 1. R 의 step 함수의 기능을 가지고 있는 패키지 등 import 합니다.
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
import pandas as pd

# 2. 데이터 불러오기
df = pd.read_csv("c:\\data\\insurance.csv", engine='python', encoding='CP949') 
print(df)

# 3. 컬럼을 인코딩합니다.
df = pd.get_dummies(df, drop_first=True)
print(df.head())   # 총 9개의 컬럼이 만들어졌음

print(df.columns.values)  # 컬럼명을 numpy array 형태로 출력
print(df.iloc[ : ,3])  # expenses

# 4. 독립변수와 종속변수를 numpy array 로 변환합니다.
#X = # 독립변수들 (numpy array 형태) --> age+sex+bim+children+smoker+region
X = df.iloc[ : ,[0,1,2,4,5,6,7,8]].to_numpy()
#y = # 종속변수 --> expenses
y = df.iloc[ : ,3].to_numpy()

# 5. 회귀 모델을 생성합니다.
model = LinearRegression()

# 6.  step 함수를 이용해서 필요한 독립변수들을 선별합니다.
#selector = RFE(회귀모델명, n_features_to_select=선별할 독립변수의 개수, step=스텝회수)
selector = RFE(model, n_features_to_select=6, step=1)

selector = selector.fit(X, y)

print(selector.support_)  # [False  True  True False  True  True  True  True]
                          # 8개의 독립변수들 중에 선택된 6개가 True 로 출력이 됨

print(selector.ranking_)  # [2 1 1 3 1 1 1 1]
                          # 8개의 독립변수들의 중요도 순위가 출력이 됨

age' 'bmi' 'children' 'sex_male' 'smoker_yes' 'region_northwest' 'region_southeast' 'region_southwest'] 의 8개의 독립변수들 중
bmi, children, smoker_yes, region_northwest, region_southeast, region_southwest 6개 변수 선택


문제26. 위의 코드를 다시 테스트 하는데 지금 추가한 파생변수가 필요한 파생변수인지 
	   확인하는 작업을 수행하시오 ! bmi30이 중요한 파생변수인지 확인하시오 !

def func_1(x):
    if x >= 30:
        return 1
    else:
        return 0

df['bmi30'] = df['bmi'].apply(func_1)
print(df)


답:
# 1. R 의 step 함수의 기능을 가지고 있는 패키지 등 import 합니다.
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
import pandas as pd

# 2. 데이터 불러오기
df = pd.read_csv("c:\\data\\insurance.csv", engine='python', encoding='CP949') 

# 3. 파생변수 추가
def func_1(x):
    if x >= 30:
        return 1
    else:
        return 0

df['bmi30'] = df['bmi'].apply(func_1)
print(df.head())

# 4. 컬럼을 인코딩합니다.
df = pd.get_dummies(df, drop_first=True)
print(df.head())   # 총 10개의 컬럼이 만들어졌음
print(df.columns.values)  # 컬럼명을 numpy array 형태로 출력


# 5. 독립변수와 종속변수를 numpy array 로 변환합니다.
#X = # 독립변수들 (numpy array 형태) --> age+sex+bim+children+smoker+region
X = df.iloc[ : ,[0,1,2,4,5,6,7,8,9]].to_numpy()
#y = # 종속변수 --> expenses
y = df.iloc[ : ,3].to_numpy()

# 6. 회귀 모델을 생성합니다.
model = LinearRegression()

# 7.  step 함수를 이용해서 필요한 독립변수들을 선별합니다.
#selector = RFE(회귀모델명, n_features_to_select=선별할 독립변수의 개수, step=스텝회수)
selector = RFE(model, n_features_to_select=6, step=1)

selector = selector.fit(X, y) 

print(selector.support_)  # [False False  True  True False  True  True  True  True]
                          # 9개의 독립변수들 중에 선택된 6개가 True 로 출력이 됨

print(selector.ranking_)  # [2 4 1 1 3 1 1 1 1]
                          # 9개의 독립변수들의 중요도 순위가 출력이 됨

['age' 'bmi' 'children' 'bmi30' 'sex_male' 'smoker_yes'
 'region_northwest' 'region_southeast' 'region_southwest'] 의 9개의 독립변수 중에
children, bmi30, smoker_yes, region_northwest, region_southeast, region_southwest
독립변수 6개 선택

bmi30은 중요한 파생변수로 선택됨.


문제27. 비만이면서 흡연을 하는 사람에 대한 파생변수를 bmi30_smoker 라는 
	   이름으로 추가하시오 !

# 1. R 의 step 함수의 기능을 가지고 있는 패키지 등 import 합니다.
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
import pandas as pd

# 2. 데이터 불러오기
df = pd.read_csv("c:\\data\\insurance.csv", engine='python', encoding='CP949') 

# 3. 파생변수 추가
def func_1(x):
    if x >= 30:
        return 1
    else:
        return 0

df['bmi30'] = df['bmi'].apply(func_1)
print(df.head())

# 4. 컬럼을 인코딩합니다.
df = pd.get_dummies(df, drop_first=True)
print(df.head())   # 총 10개의 컬럼이 만들어졌음
print(df.columns.values)  # 컬럼명을 numpy array 형태로 출력

df['bmi30_smoker'] = df['bmi30']*df['smoker_yes'] 

# 5. 독립변수와 종속변수를 numpy array 로 변환합니다.
#X = # 독립변수들 (numpy array 형태) 
X = df.iloc[ : ,[0,1,2,4,5,6,7,8,9,10]].to_numpy()
#y = # 종속변수 --> expenses
y = df.iloc[ : ,3].to_numpy()

# 6. 회귀 모델을 생성합니다.
model = LinearRegression()

# 7.  RFE(후진제거법) 함수를 이용해서 필요한 독립변수들을 선별합니다.
#selector = RFE(회귀모델명, n_features_to_select=선별할 독립변수의 개수, step=한번에 제거할 개수)
selector = RFE(model, n_features_to_select=6, step=1)

selector = selector.fit(X, y) 

print(selector.support_)  # [False False  True False  True  True False  True  True  True]
                          # 10개의 독립변수들 중에 선택된 6개가 True 로 출력이 됨

print(selector.ranking_)  # [3 5 1 4 1 1 2 1 1 1]
                          # 10개의 독립변수들의 중요도 순위가 출력이 됨

변수선택:
['children'   'sex_male' 'smoker_yes' 'region_southeast' 'region_southwest' 'bmi30_smoker']


-------------------------------------------------------------------------------------------------------
# 학습을 마친 모형에 test data를 적용하여 결정계수(R-제곱) 계산
model.fit(X, y)
r_square = model.score(X, y)

print(r_square)
print('\n')

# 회귀식의 기울기
print('기울기 a: ', model.coef_)
print('\n')

# 회귀식의 y절편
print('y절편 b', model.intercept_)
print('\n')

결과:
0.8638704460994961

기울기 a:  [  263.24284784   114.82824556   520.4738381   -863.25462888
  -491.12953961 13402.28725234  -266.79968469  -824.57297575
 -1223.8696938  19794.26437576]

y절편 b -4740.682154731916

-------------------------------------------------전체 독립변수 회귀 결과
model = smf.ols('expenses ~ age+bmi+children+bmi30+sex_male+smoker_yes+region_northwest+region_southeast+region_southwest+bmi30_smoker', data=df)
result = model.fit()
print( result.summary() ) 


OLS Regression Results                            
==============================================================================
Dep. Variable:               expenses   R-squared:                       0.864
Model:                            OLS   Adj. R-squared:                  0.863
Method:                 Least Squares   F-statistic:                     842.1
Date:                Wed, 24 Feb 2021   Prob (F-statistic):               0.00
Time:                        17:11:25   Log-Likelihood:                -13144.
No. Observations:                1338   AIC:                         2.631e+04
Df Residuals:                    1327   BIC:                         2.637e+04
Df Model:                          10                                         
Covariance Type:            nonrobust                                         
====================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------------
Intercept        -4740.6822    960.019     -4.938      0.000   -6624.002   -2857.362
age                263.2428      8.805     29.896      0.000     245.969     280.517
bmi                114.8282     34.574      3.321      0.001      47.003     182.653
children           520.4738    101.959      5.105      0.000     320.455     720.493
bmi30             -863.2546    425.900     -2.027      0.043   -1698.765     -27.744
sex_male          -491.1295    246.568     -1.992      0.047    -974.834      -7.425
smoker_yes         1.34e+04    443.918     30.191      0.000    1.25e+04    1.43e+04
region_northwest  -266.7997    352.417     -0.757      0.449    -958.154     424.555
region_southeast  -824.5730    354.812     -2.324      0.020   -1520.627    -128.519
region_southwest -1223.8697    353.685     -3.460      0.001   -1917.713    -530.026
bmi30_smoker      1.979e+04    610.110     32.444      0.000    1.86e+04     2.1e+04
==============================================================================
Omnibus:                      871.061   Durbin-Watson:                   2.060
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7318.497
Skew:                           3.087   Prob(JB):                         0.00
Kurtosis:                      12.652   Cond. No.                         420.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

----------------------------------------------------
-----------------------------------------------------------------------------------------
# 선택된 변수로 구한 결과
r_square = selector.score(X, y)

print(r_square)
print('\n')

# 회귀식의 기울기
print('기울기 a: ', model.coef_)
print('\n')

# 회귀식의 y절편
print('y절편 b', model.intercept_)
print('\n')

0.7676628662247236


기울기 a:  [  263.24284784   114.82824556   520.4738381   -863.25462888
  -491.12953961 13402.28725234  -266.79968469  -824.57297575
 -1223.8696938  19794.26437576]


y절편 b -4740.682154731916

-----------------------------------------------------------------------------------------

문제28. (오늘의 마지막 문제) 문제 27번에서 선별된 독립변수들만 가지고 회귀모델을 생성하고
	   훈련시켜 나온 결정계수가 어떻게 되는지 확인하시오 !

힌트:
X = ?
y = ? 
model.fit(X, y)
r_square = model.score(X, y)

print(r_square)


#
import statsmodels.formula.api as smf

model = smf.ols('expenses ~ children + sex_male + smoker_yes + region_southeast + region_southwest + bmi30_smoker', data=df)
result = model.fit()
print( result.summary() )  # 결정계수 : 0.768


                            OLS Regression Results                            
==============================================================================
Dep. Variable:               expenses   R-squared:                       0.768
Model:                            OLS   Adj. R-squared:                  0.767
Method:                 Least Squares   F-statistic:                     733.0
Date:                Wed, 24 Feb 2021   Prob (F-statistic):               0.00
Time:                        17:06:41   Log-Likelihood:                -13501.
No. Observations:                1338   AIC:                         2.702e+04
Df Residuals:                    1331   BIC:                         2.705e+04
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
====================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------------
Intercept         8396.1342    321.308     26.131      0.000    7765.810    9026.459
children           655.5237    132.802      4.936      0.000     394.999     916.048
sex_male          -631.9316    321.440     -1.966      0.050   -1262.515      -1.348
smoker_yes        1.285e+04    546.291     23.517      0.000    1.18e+04    1.39e+04
region_southeast  -487.8482    385.419     -1.266      0.206   -1243.942     268.246
region_southwest  -973.5635    398.420     -2.444      0.015   -1755.164    -191.963
bmi30_smoker      2.043e+04    712.593     28.672      0.000     1.9e+04    2.18e+04
==============================================================================
Omnibus:                      378.619   Durbin-Watson:                   2.061
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              978.911
Skew:                           1.491   Prob(JB):                    2.71e-213
Kurtosis:                       5.943   Cond. No.                         9.77
==============================================================================
